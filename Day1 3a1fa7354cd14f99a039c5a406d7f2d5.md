# Day1

## Tensor

### 创建tensor

[常见构造Tensor的方法](https://www.notion.so/a290be194d974799ae76236be8729d29)

### 张量的操作

- 加法操作：add_进行原值修改
- 索引操作：**索引出来的结果与原数据共享内存，修改一个，另一个会跟着修改。如果不想修改，可以考虑使用copy()等方法**
- 维度变换
    - torch.view:返回的新`tensor`与源`tensor`共享内存(其实是同一个`tensor`)，更改其中的一个，另外一个也会跟着改变。
    - clone: 创造一个张量副本然后再使用 `torch.view()`进行函数维度变换 。
    - 取值操作：如果我们有一个元素 `tensor` ，我们可以使用 `.item()`来获得这个 `value`，而不获得其他性质

### 广播机制

当对两个形状不同的 Tensor 按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算。

## 自动求导

- torch.tensor 设置属性 .requires_grad=True 那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 `.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。
    - 注意：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。
    - 要阻止一个张量被跟踪历史，可以调用`.detach()`方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。
    - `a**.**requires_grad_**(True)`** 原地改变张量requires_grad的标志
- 还有一个类对于`autograd`的实现非常重要：`Function`。`Tensor` 和 `Function`互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个`.grad_fn`
属性。除非该张量是人为定义的。

### 梯度

注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。

- 通过将代码块包装在 `with torch.no_grad():`中，来阻止 autograd 跟踪设置了`.requires_grad=True`的张量的历史记录。

```python
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)

True
True
False
```

- 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我们可以对 tensor.data 进行操作。

```python
x = torch.ones(1,requires_grad=True)

print(x.data) # 还是一个tensor
print(x.data.requires_grad) # 但是已经是独立于计算图之外

y = 2 * x
x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播

y.backward()
print(x) # 更改data的值也会影响tensor的值 
print(x.grad)
```

```python
tensor([1.])
False
tensor([100.], requires_grad=True) #Tensor的值改变了
tensor([2.])#tensor求导的值没有受到100的变化
```

## 并行计算

在编写程序中，当我们使用了 `.cuda()`时，其功能是让我们的模型或者数据从CPU迁移到GPU(0)当中，通过GPU开始计算。

### **一层的任务分布到不同数据中(Layer-wise partitioning)**

同一层的模型做一个拆分，让不同的GPU去训练同一层模型的部分任务。

### **不同的数据分布到不同的设备中，执行相同的任务(Data parallelism)**

所谓的拆分数据就是，同一个模型在不同GPU中训练一部分数据，然后再分别计算一部分数据之后，只需要将输出的数据做一个汇总，然后再反传。